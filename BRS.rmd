---
title: "Final Project"
author: "Ankit Prakash"
date: "26 November 2018"
output:
  html_document: default
  pdf_document: default
  word_document: default
---

```{r }
library(recommenderlab)
library(data.table)
library(dplyr)
library(tidyr)
library(ggplot2)
library(stringr)
library(DT)
library(knitr)
library(grid)
library(gridExtra)
library(corrplot)
library(qgraph)
library(methods)
library(Matrix)
library(sqldf)
```

```{r}
setwd("~/Documents/New Data")
books <- fread('books.csv')

book_new <- books %>% select(isbn, original_title,authors,original_publication_year,language_code)
names(book_new) <- c('ISBN', 'bookTitle', 'bookAuthor', 'yearOfPublication', 'language')
head(book_new)
```

Reading Ratings Data
```{r}
setwd("~/Documents/New Data")
ratings <- fread('ratings.csv')
head(ratings)
dim(ratings)
```

Reading Users Data
```{r}
setwd("~/Documents/New Data")
users <- read.csv("users.csv", stringsAsFactors = F, na.strings = "")
names(users) <- c('user_id', 'Area', 'Location', 'Country', 'Age')
head(users)
```

# Cleaning the users data
```{r }
# To check the data types of the variable
str(users)
# User ID
# To check unique values of user ID
user_id_dup <- sqldf("select user_id, count(user_id) from users group by user_id having count(user_id)>1")
nrow(user_id_dup)
#As it can be seen that these are nrow() returns 0 that means user_id are unique

# Age
#Converting Age as numeric value
users$Age <- as.numeric(users$Age)
sort(unique(users$Age), decreasing = F)

#In my view values below 5 and above 90 do not make much sense for our book rating case...hence replacing these by NA
users$Age[(users$Age > 90) | (users$Age < 5)] <- NA

#replacing NaNs with mean
mean_age <- round(mean(users$Age, na.rm = T),0)
users$Age[is.na(users$Age)] <- mean_age

#Rechecking for the Unique Age which should lie between 5 years to 90 Years
sort(unique(users$Age), decreasing = F)
```

The data contains nearly 1mio rows, so for this step I found data.table to be significantly faster than dplyr. If you have not yet tried it out, I recommend you to do so. It helped me a lot e.g., in the Instacart competition. 
So let’s first remove the duplicate ratings.
```{r }
ratings[, N := .N, .(user_id, book_id)]
## corresponding dplyr code
#ratings %>% group_by(user_id, book_id) %>% mutate(n=n())
cat('Number of duplicate ratings: ', nrow(ratings[N > 1]))
## Number of duplicate ratings:  4487
ratings <- ratings[N == 1]

#And then let’s remove users who rated fewer than 3 books.
ratings[, N := .N, .(user_id)]
## corresponding dplyr code
#ratings %>% group_by(user_id) %>% mutate(n = n())
cat('Number of users who rated fewer than 3 books: ', uniqueN(ratings[N <= 2, user_id]))
```

Select a subset of users
To reduce calculation times in this kernel, I select only a subset of users. (e.g., 20%)
```{r }
set.seed(1)
user_fraction <- 0.2
users <- unique(ratings$user_id)
sample_users <- sample(users, round(user_fraction * length(users)))
 
cat('Number of ratings (before): ', nrow(ratings),'\n')

ratings <- ratings[ratings$user_id %in% sample_users,]
cat('Number of ratings (after): ', nrow(ratings))
```

```{r }
ratings %>% 
  ggplot(aes(x = rating, fill = factor(rating))) +
  geom_bar(color = "grey20") + scale_fill_brewer(palette = "YlGnBur") + guides(fill = FALSE)

```

```{r }
ratings %>% 
  group_by(user_id) %>% 
  summarize(number_of_ratings_per_user = n()) %>% 
  ggplot(aes(number_of_ratings_per_user)) + 
  geom_bar(fill = "cadetblue3", color = "grey20") + coord_cartesian(c(3, 50))
```

```{r }
ratings %>% 
  group_by(user_id) %>% 
  summarize(mean_user_rating = mean(rating)) %>% 
  ggplot(aes(mean_user_rating)) +
  geom_histogram(fill = "cadetblue3", color = "grey20")
```

```{r }
ratings %>% 
  group_by(book_id) %>% 
  summarize(number_of_ratings_per_book = n()) %>% 
  ggplot(aes(number_of_ratings_per_book)) + 
  geom_bar(fill = "orange", color = "grey20", width = 1) + coord_cartesian(c(0,40))
```


```{r }
ratings %>% 
  group_by(book_id) %>% 
  summarize(mean_book_rating = mean(rating)) %>% 
  ggplot(aes(mean_book_rating)) + geom_histogram(fill = "orange", color = "grey20") + coord_cartesian(c(1,5))
```

Different languages
You might have seen in the books.csv that there is language information on the books. This is interesting because goodreads is an english speaking site. However, the dataset contains some books in different languages. The reason is that typically there are multiple editions of a book (both in the same language and in different languages). For this dataset it seems that the most popular edition was included, which for some books is their original language.
```{r}
p1 <- books %>% 
  mutate(language = factor(language_code)) %>% 
  group_by(language) %>% 
  summarize(number_of_books = n()) %>% 
  arrange(-number_of_books) %>% 
  ggplot(aes(reorder(language, number_of_books), number_of_books, fill = reorder(language, number_of_books))) +
  geom_bar(stat = "identity", color = "grey20", size = 0.35) + coord_flip() +
  labs(x = "language", title = "english included") + guides(fill = FALSE)

p2 <- books %>% 
  mutate(language = factor(language_code)) %>% 
  filter(!language %in% c("en-US", "en-GB", "eng", "en-CA", "")) %>% 
  group_by(language) %>% 
  summarize(number_of_books = n()) %>% 
  arrange(-number_of_books) %>% 
  ggplot(aes(reorder(language, number_of_books), number_of_books, fill = reorder(language, number_of_books))) +
  geom_bar(stat = "identity", color = "grey20", size = 0.35) + coord_flip() +
  labs(x = "", title = "english excluded") + guides(fill = FALSE)

grid.arrange(p1,p2, ncol=2)
```

Top 10 rated books
It is apparent that users seem to like a) Calvin and Hobbes in general, b) compilations of books. This makes sense intuitively as people won't get interested in an entire compilation if they don't like the individual books.
```{r }
books %>%
  arrange(-average_rating) %>% 
  top_n(10,wt = average_rating) %>% 
  select(title, ratings_count, average_rating) %>% 
  datatable(class = "nowrap hover row-border", escape = FALSE, options = list(dom = 't',scrollX = TRUE, autoWidth = TRUE))
```
# Top 10 popular books
# By looking at the books that were rated most often we can get an impression of the popularity of a book. #You can see the top 10 popular books in the table below.
```{r }
books %>% 
  arrange(-ratings_count) %>% 
  top_n(10,wt = ratings_count) %>% 
  select(title, ratings_count, average_rating) %>% 
  datatable(class = "nowrap hover row-border", escape = FALSE, options = list(dom = 't',scrollX = TRUE, autoWidth = TRUE))
```

```{r }
tmp <- books %>% 
  select(one_of(c("books_count","original_publication_year","ratings_count", "work_ratings_count", "work_text_reviews_count", "average_rating"))) %>% 
  as.matrix()

corrplot(cor(tmp, use = 'pairwise.complete.obs'), type = "lower")
```

Is there a relationship between the number of ratings and the average rating?
Theoretically, it might be that the popularity of a book (in terms of the number of ratings it receives) is associated with the average rating it receives, such that once a book is becoming popular it gets better ratings. However, our data shows that this is true only to a very small extent. The correlation between these variables is only 0.045.
```{r }
get_cor <- function(df){
    m <- cor(df$x,df$y, use="pairwise.complete.obs");
    eq <- substitute(italic(r) == cor, list(cor = format(m, digits = 2)))
    as.character(as.expression(eq));                 
}
books %>% 
  filter(ratings_count < 1e+5) %>% 
  ggplot(aes(ratings_count, average_rating)) + stat_bin_hex(bins = 50) + scale_fill_distiller(palette = "Spectral") + 
  stat_smooth(method = "lm", color = "orchid", size = 2) +
  annotate("text", x = 85000, y = 2.7, label = get_cor(data.frame(x = books$ratings_count, y = books$average_rating)), parse = TRUE, color = "orchid", size = 7)
```

Do frequent raters rate differently?
It is possible, that users that rate more books (frequent raters) rate books differently from less frequent raters. The figure below explores this possibility. It seems like frequent raters tend to give lower ratings to books, maybe they are/become more critical the more they read and rate. That's interesting.
```{r}
tmp <- ratings %>% 
  group_by(user_id) %>% 
  summarize(mean_rating = mean(rating), number_of_rated_books = n())

tmp %>% filter(number_of_rated_books <= 100) %>% 
  ggplot(aes(number_of_rated_books, mean_rating)) + stat_bin_hex(bins = 50) + scale_fill_distiller(palette = "Spectral") + stat_smooth(method = "lm", color = "orchid", size = 2, se = FALSE) +
  annotate("text", x = 80, y = 1.9, label = get_cor(data.frame(x = tmp$number_of_rated_books, y = tmp$mean_rating)), color = "orchid", size = 7, parse = TRUE)
```

Series of books
The data contains information in the title column about whether a certain book is part of a series (e.g. the Lord of the Rings trilogy).

```{r }
books %>% 
  filter(str_detect(str_to_lower(title), '\\(the lord of the rings')) %>% 
  select(book_id, title, average_rating) %>% 
  datatable(class="nowrap hover row-border", options = list(dom = 't',scrollX = TRUE, autoWidth = TRUE))
```

Given this, we can extract the series from the title, as it is always given in parantheses. Wen can then calculate features like the number of volumes in a series, and so forth.

Below, I examine whether books which are part of a larger series receive a higher rating. In fact the more volumes are in a series, the higher the average rating is.
```{r}
books <- books %>% 
  mutate(series = str_extract(title, "\\(.*\\)"), 
         series_number = as.numeric(str_sub(str_extract(series, ', #[0-9]+\\)$'),4,-2)),
         series_name = str_sub(str_extract(series, '\\(.*,'),2,-2))

tmp <- books %>% 
  filter(!is.na(series_name) & !is.na(series_number)) %>% 
  group_by(series_name) %>% 
  summarise(number_of_volumes_in_series = n(), mean_rating = mean(average_rating))
  
tmp %>% 
  ggplot(aes(number_of_volumes_in_series, mean_rating)) + stat_bin_hex(bins = 50) + scale_fill_distiller(palette = "Spectral") +
  stat_smooth(method = "lm", se = FALSE, size = 2, color = "orchid") +
  annotate("text", x = 35, y = 3.95, label = get_cor(data.frame(x = tmp$mean_rating,  y = tmp$number_of_volumes_in_series)), color = "orchid", size = 7, parse = TRUE)
```

Is the sequel better than the original?
We can also see that within a series, in fact the sequel is rated slightly better than the original.
```{r}
books %>% 
  filter(!is.na(series_name) & !is.na(series_number) & series_number %in% c(1,2)) %>% 
  group_by(series_name, series_number) %>% 
  summarise(m = mean(average_rating)) %>% 
  ungroup() %>% 
  group_by(series_name) %>% 
  mutate(n = n()) %>% 
  filter(n == 2) %>% 
  ggplot(aes(factor(series_number), m, color = factor(series_number))) +
  geom_boxplot() + coord_cartesian(ylim = c(3,5)) + guides(color = FALSE) + labs(x = "Volume of series", y = "Average rating") 

```
Does the number of authors matter?
We all know the saying: "too many cooks spoil the broth." Is this also true for books? Looking at the plot below it seems to be exactly the opposite: The more authors a book has the higher is its average rating.
```{r}
books <- books %>% 
  group_by(book_id) %>% 
  mutate(number_of_authors = length(str_split(authors, ",")[[1]]))

books %>% filter(number_of_authors <= 10) %>% 
  ggplot(aes(number_of_authors, average_rating)) + stat_bin_hex(bins = 50) + scale_fill_distiller(palette = "Spectral") +
  stat_smooth(method = "lm", size = 2, color = "orchid", se = FALSE) + 
  annotate("text", x = 8.5, y = 2.75, label = get_cor(data.frame(x = books$number_of_authors, y = books$average_rating)), color = "orchid", size = 7, parse = TRUE)
```

Part II: Collaborative Filtering
Popularity based Recommendation System
```{r }
colnames(ratings)
ratings_count = ratings %>% group_by(book_id) %>% summarise(rating_sum = sum(rating)) %>% arrange(-rating_sum) %>% head(10)
df <- merge(ratings_count, (books %>% select(book_id, original_title,authors,original_publication_year,language_code)), by.x = 'book_id', by.y = "book_id", all.x = T)
df$rating_sum <- NULL
print("Following books are recommended")
df %>% select(book_id, original_title,authors,original_publication_year,language_code)
```

By Using rating matrix for UBCF and IBCF
```{r }
gc()
dimension_names <- list(user_id = sort(unique(ratings$user_id)), book_id = sort(unique(ratings$book_id)))
ratingmat <- spread(select(ratings, book_id, user_id, rating), book_id, rating) %>% select(-user_id)

ratingmat <- as.matrix(ratingmat)
dimnames(ratingmat) <- dimension_names
ratingmat[1:5, 1:5]
```
UBCF
```{r}
library(recommenderlab)
ratingmat <- spread(select(ratings, book_id, user_id, rating), book_id, rating) %>% select(-user_id)
ratingmat <- as(as.matrix(ratingmat[,-1], 'realRatingMatrix'))
dimnames(ratingmat) <- dimension_names
model <- Recommender(ratingmat, method = "UBCF", param = list( method= "Cosine"))
result = predict(model, ratingmat[1,], n = 10)
books_rec <- as.numeric(as(result, "list")[[1]])
books %>% filter(book_id %in% books_rec) %>% select(book_id,original_title)
```

Step 1: Find similar users
For this step we select users that have in common that they rated the same books. To make it easier let's select one example user "David" (user_id: 17329). First we select users that rated at least one book that David also rated. In total there are 440 users who have at least one book in common.
```{r }
current_user <- "17329"
rated_items <- which(!is.na((as.data.frame(ratingmat[current_user, ]))))
selected_users <- names(which(apply(!is.na(ratingmat[ ,rated_items]), 1, sum) >= 2))
head(selected_users, 40)
```
For these users, we can calculate the similarity of their ratings with “David” s ratings. There is a number of options to calculate similarity. Typically cosine similarity or pearson’s correlation coefficient are used. Here, I chose pearson’s correlation. We would now go through all the selected users and calculate the similarity between their and David’s ratings. Below I do this for 2 users (user_ids: 1339 and 21877) for illustration. We can see that similarity is higher for user 1339 than user 21877
```{r }
user1 <- data.frame(item=colnames(ratingmat),rating=ratingmat[current_user,]) %>% filter(!is.na(rating))
user2 <- data.frame(item=colnames(ratingmat),rating=ratingmat["1323",]) %>% filter(!is.na(rating))
tmp<-merge(user1, user2, by="item")
tmp
```

```{r }
cor(tmp$rating.x, tmp$rating.y, use="pairwise.complete.obs")
```

```{r }
user2 <- data.frame(item = colnames(ratingmat), rating = ratingmat["21877", ]) %>% filter(!is.na(rating))
tmp <- merge(user1, user2, by="item")
tmp
```


```{r }
cor(tmp$rating.x, tmp$rating.y, use="pairwise.complete.obs")
```
For these users, we can calculate the similarity of their ratings with “David” s ratings. There is a number of options to calculate similarity. Typically cosine similarity or pearson’s correlation coefficient are used. Here, I chose pearson’s correlation. We would now go through all the selected users and calculate the similarity between their and David’s ratings. Below I do this for 2 users (user_ids: 1339 and 21877) for illustration. We can see that similarity is higher for user 1339 than user 21877
```{r }
rmat <- ratingmat[selected_users, ]
user_mean_ratings <- rowMeans(rmat,na.rm=T)
rmat <- rmat - user_mean_ratings
```

We can calculate the similarity of all others users with David and sort them according to the highest similarity.
```{r }
similarities <- cor(t(rmat[rownames(rmat)!=current_user, ]), rmat[current_user, ], use = 'pairwise.complete.obs')
sim <- as.vector(similarities)
names(sim) <- rownames(similarities)
res <- sort(sim, decreasing = TRUE)
head(res, 40)
```

Visualizing similarities between users
Similarities between users can be visualized using the qpraph package. The width of the graph's edges correspond to similarity (blue for positive correlations, red for negative correlations).
```{r}
sim_mat <- cor(t(rmat), use = 'pairwise.complete.obs')
random_users <- selected_users[1:20]
qgraph(sim_mat[c(current_user, random_users), c(current_user, random_users)], layout = "spring", vsize = 5, theme = "TeamFortress", labels = c(current_user, random_users))
```
Step 2: Get predictions for other books
In order to get recommendations for our user we would take the most similar users (e.g. 4) and average their ratings for books David has not yet rated. To make these averages more reliable you could also only include items that have been rated by multiple other similar users.
```{r}
similar_users <- names(res[1:4])

similar_users_ratings <- data.frame(item = rep(colnames(rmat), length(similar_users)), rating = c(t(as.data.frame(rmat[similar_users,])))) %>% filter(!is.na(rating))

current_user_ratings <- data.frame(item = colnames(rmat), rating = rmat[current_user,]) %>% filter(!is.na(rating))

predictions <- similar_users_ratings %>% 
  filter(!(item %in% current_user_ratings$item)) %>% 
  group_by(item) %>% summarize(mean_rating = mean(rating))

predictions
```
Step 3: Recommend the best 5 predictions
Given the results, we would sort the predictions with respect to their mean rating and recommend the highest rated books to David. In our case that would be books 1031, 2004, 3934, 5524, 7239. Please note that ratings are still normalized.
```{r}
predictions %>% 
  arrange(-mean_rating) %>% 
  top_n(5, wt = mean_rating) %>% 
  mutate(book_id = as.numeric(as.character(item))) %>% 
  left_join(select(books, authors, title, book_id), by = "book_id") %>% 
  select(-item) 
```
Using recommenderlab
Recommenderlab is a R-package that provides the infrastructure to evaluate and compare several collaborative-filtering algortihms. Many algorithms are already implemented in the package, and we can use the available ones to save some coding effort, or add custom algorithms and use the infrastructure (e.g. crossvalidation).

There is an important aspect concerning the representation of our rating matrix. As we could already see above, most of the values in the rating matrix are missing, because every user just rated a few of the 10000 books. This allows us to represent this matrix is sparse format in order to save memory.
```{r}
ratingmat0 <- ratingmat
ratingmat0[is.na(ratingmat0)] <- 0
sparse_ratings <- as(ratingmat0, "sparseMatrix")
rm(ratingmat0)
gc()
```

Recommenderlab uses as special variant of a sparse matrices, so we convert to this class first.
```{r}
real_ratings <- new("realRatingMatrix", data = sparse_ratings)
real_ratings

```
UBCF
```{r }
model <- Recommender(real_ratings, method = "UBCF", param = list(method = "pearson", nn = 4))
```


```{r }
prediction <- predict(model, real_ratings[current_user, ], type = "ratings")
# Let's have a look at the best predictions for David:
as(prediction, 'data.frame') %>% 
  arrange(-rating) %>% .[1:5,] %>% 
  mutate(book_id = as.numeric(as.character(item))) %>% 
  left_join(select(books, authors, title, book_id), by = "book_id") %>% 
  select(-item) 
```


```{r }
scheme <- evaluationScheme(real_ratings[1:500,], method = "cross-validation", k = 10, given = -1, goodRating = 5)
```

```{r}
algorithms <- list("random" = list(name = "RANDOM", param = NULL),
                   "UBCF_05" = list(name = "UBCF", param = list(nn = 5)),
                   "UBCF_10" = list(name = "UBCF", param = list(nn = 10)),
                   "UBCF_30" = list(name = "UBCF", param = list(nn = 30)),                   
                   "UBCF_50" = list(name = "UBCF", param = list(nn = 50))
                   )
# evaluate the alogrithms with the given scheme            
results <- evaluate(scheme, algorithms, type = "ratings")
```


```{r}
# restructure results output
tmp <- lapply(results, function(x) slot(x, "results"))
res <- tmp %>% 
  lapply(function(x) unlist(lapply(x, function(x) unlist(x@cm[ ,"RMSE"])))) %>% 
  as.data.frame() %>% 
  gather(key = "Algorithm", value = "RMSE")

res %>% 
  ggplot(aes(Algorithm, RMSE, fill = Algorithm)) +
  geom_bar(stat = "summary") + geom_errorbar(stat = "summary", width = 0.3, size = 0.8) +
  coord_cartesian(ylim = c(0.6, 1.3)) + guides(fill = FALSE)

```

```{r}
recommenderRegistry$get_entry_names()
``` 

```{r}
recommenderRegistry$get_entries(dataType = "realRatingMatrix")
``` 

```{r}
scheme <- evaluationScheme(real_ratings[1:500,], method = "cross-validation", k = 10, given = -1, goodRating = 5)

algorithms <- list("random" = list(name = "RANDOM", param = NULL),
                   "popular" = list(name = "POPULAR"),
                   "UBCF" = list(name = "UBCF"),
                   "SVD" = list(name = "SVD")
                   )
                   
results <- evaluate(scheme, algorithms, type = "ratings", progress = FALSE)
```

```{r}
# restructure results output
tmp <- lapply(results, function(x) slot(x, "results"))
res <- tmp %>% 
  lapply(function(x) unlist(lapply(x, function(x) unlist(x@cm[ ,"RMSE"])))) %>% 
  as.data.frame() %>% 
  gather(key = "Algorithm", value = "RMSE")

res %>% 
  mutate(Algorithm=factor(Algorithm, levels = c("random", "popular", "UBCF", "SVD"))) %>%
  ggplot(aes(Algorithm, RMSE, fill = Algorithm)) + geom_bar(stat = "summary") + 
  geom_errorbar(stat = "summary", width = 0.3, size = 0.8) + coord_cartesian(ylim = c(0.6, 1.3)) + 
  guides(fill = FALSE)
```

```{r}

```

```{r}

```

```{r}

```
